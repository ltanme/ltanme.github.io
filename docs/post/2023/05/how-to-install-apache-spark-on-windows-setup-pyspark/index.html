<!DOCTYPE html>
<html lang="en">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=55668&amp;path=livereload" data-no-instant defer></script>
  
    <title>How to Install Apache Spark on Windows Setup PySpark :: LTAN.ME</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="为什么要在Windows 10上运行Scala Spark程序 开发环境设置简单： 对于许多开发者来说，Windows是他们最熟悉的操作系统， 因此在Windows上进行开发可以节省大量的环境设置和配置时间。 此外，Windows上有IntelliJ IDEA可以方便Scala和Spark的开发。
" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="http://localhost:55668/post/2023/05/how-to-install-apache-spark-on-windows-setup-pyspark/" />




<link rel="stylesheet" href="http://localhost:55668/assets/style.css">

  <link rel="stylesheet" href="http://localhost:55668/assets/green.css">






<link rel="apple-touch-icon" href="http://localhost:55668/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="http://localhost:55668/img/favicon/green.png">



<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="How to Install Apache Spark on Windows Setup PySpark">
<meta property="og:description" content="为什么要在Windows 10上运行Scala Spark程序 开发环境设置简单： 对于许多开发者来说，Windows是他们最熟悉的操作系统， 因此在Windows上进行开发可以节省大量的环境设置和配置时间。 此外，Windows上有IntelliJ IDEA可以方便Scala和Spark的开发。
" />
<meta property="og:url" content="http://localhost:55668/post/2023/05/how-to-install-apache-spark-on-windows-setup-pyspark/" />
<meta property="og:site_name" content="LTAN.ME" />

  
    <meta property="og:image" content="http://localhost:55668/img/favicon/green.png">
  

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">


  <meta property="article:published_time" content="2023-05-11 15:25:41 &#43;0800 CST" />












</head>
<body class="green">


<div class="container center headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    LTAN.ME
  </div>
</a>

    </div>
    
    <div class="menu-trigger">menu</div>
    
  </div>
  
  <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/">Home</a></li>
        
      
        
          <li><a href="/post/">Archives</a></li>
        
      
        
          <li><a href="/tags/">Tags</a></li>
        
      
        
          <li><a href="/about/">About</a></li>
        
      
      
    

    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/">Home</a></li>
      
    
      
        <li><a href="/post/">Archives</a></li>
      
    
      
        <li><a href="/tags/">Tags</a></li>
      
    
      
        <li><a href="/about/">About</a></li>
      
    
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="http://localhost:55668/post/2023/05/how-to-install-apache-spark-on-windows-setup-pyspark/">How to Install Apache Spark on Windows Setup PySpark</a></h1>
  <div class="post-meta">
    
      <span class="post-date">
        2023-05-11 [Mod: 2023-05-11]
      </span>
    
    
  </div>

  
  <span class="post-tags">
    
    #<a href="http://localhost:55668/tags/python3/">python3</a>&nbsp;
    
    #<a href="http://localhost:55668/tags/pyspark/">pyspark</a>&nbsp;
    
    #<a href="http://localhost:55668/tags/scala/">scala</a>&nbsp;
    
    #<a href="http://localhost:55668/tags/winutils/">winutils</a>&nbsp;
    
  </span>
  

  

  

  <div class="post-content"><div>
        <h2 id="为什么要在windows-10上运行scala-spark程序">为什么要在Windows 10上运行Scala Spark程序<a href="#为什么要在windows-10上运行scala-spark程序" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<h3 id="开发环境设置简单">开发环境设置简单：<a href="#开发环境设置简单" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>对于许多开发者来说，Windows是他们最熟悉的操作系统，
因此在Windows上进行开发可以节省大量的环境设置和配置时间。
此外，Windows上有IntelliJ IDEA可以方便Scala和Spark的开发。</p>
<h3 id="本地测试方便">本地测试方便：<a href="#本地测试方便" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>在本地Windows环境中进行开发，
可以方便快速的进行代码的单元测试和调试。
虽然Spark在集群中运行时的行为可能与在单个机器上有所不同，
但对于许多常见的任务，
本地测试通常可以提供足够的保证。</p>
<h2 id="如何做">如何做<a href="#如何做" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<h3 id="步骤1下载spark并安装">步骤1，下载spark并安装<a href="#步骤1下载spark并安装" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>根据自己的环境版本，在官网下载<code>spark-3.2.4-bin-hadoop2.7</code>包
下载链接地址为<code>https://spark.apache.org/downloads.html</code>
解压.tgz文件放到你本地目录夹,如<code>d:\spark-2.4.4-bin-hadoop2.7</code></p>
<h3 id="步骤2-下载winutils并安装">步骤2 下载winutils并安装<a href="#步骤2-下载winutils并安装" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>下载winutils.exe(解释windows hadoop通信问题)
下载链接地址<code>https://github.com/steveloughran/winutils/blob/master/hadoop-2.7.1/bin/winutils.exe</code>
存放到<code>d:\spark-2.4.4-bin-hadoop2.7\bin\winutils.exe</code> 该目录下</p>
<h3 id="步骤3设置环境变量">步骤3，设置环境变量<a href="#步骤3设置环境变量" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>设置windows环境变量</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># Environment variable: </span>
</span></span><span style="display:flex;"><span>SPARK_HOME<span style="color:#f92672">=</span>d:<span style="color:#ae81ff">\s</span>park-2.4.4-bin-hadoop2.7
</span></span><span style="display:flex;"><span>HADOOP_HOME<span style="color:#f92672">=</span>d:<span style="color:#ae81ff">\s</span>park-2.4.4-bin-hadoop2.7
</span></span><span style="display:flex;"><span><span style="color:#75715e"># PATH variable:</span>
</span></span><span style="display:flex;"><span> d:<span style="color:#ae81ff">\s</span>park-2.4.4-bin-hadoop2.7<span style="color:#ae81ff">\b</span>in
</span></span></code></pre></div><h3 id="步骤4-验证测试scala-spark--pyspark">步骤4 验证测试scala spark 、 pyspark<a href="#步骤4-验证测试scala-spark--pyspark" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>测试 pyspark</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> sys
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 这里不需要用pip install pyspark 会容易报错，根本不好安装的，不好使还不如这样引用</span>
</span></span><span style="display:flex;"><span>sys<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#34;D:\spark-3.2.4-bin-hadoop2.7\python&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql <span style="color:#f92672">import</span> SparkSession
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 创建 SparkSession</span>
</span></span><span style="display:flex;"><span>spark <span style="color:#f92672">=</span> SparkSession<span style="color:#f92672">.</span>builder
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>appName(<span style="color:#e6db74">&#34;HDFS Read Example&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>config(<span style="color:#e6db74">&#34;spark.master&#34;</span>, <span style="color:#e6db74">&#34;local&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>getOrCreate()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 读取 CSV 文件</span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>read<span style="color:#f92672">.</span>csv(<span style="color:#e6db74">&#34;hdfs://sybigdata/user/example/part-00000-d3d3436e-f258-43b8-bfec-e45eb966edc7-c000.csv&#34;</span>, header<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 显示前 10 行</span>
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>show(<span style="color:#ae81ff">10</span>)
</span></span></code></pre></div><p>查看显示结果</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>Using Spark<span style="color:#960050;background-color:#1e0010">&#39;</span>s default log4j profile: org/apache/spark/log4j-defaults.properties
</span></span><span style="display:flex;"><span>Setting default log level to <span style="color:#e6db74">&#34;WARN&#34;</span>.
</span></span><span style="display:flex;"><span>To adjust logging level use sc.setLogLevel<span style="color:#f92672">(</span>newLevel<span style="color:#f92672">)</span>. For SparkR, use setLogLevel<span style="color:#f92672">(</span>newLevel<span style="color:#f92672">)</span>.
</span></span><span style="display:flex;"><span>23/05/11 15:19:23 WARN NativeCodeLoader: Unable to load native-hadoop library <span style="color:#66d9ef">for</span> your platform... using builtin-java classes where applicable
</span></span><span style="display:flex;"><span>+------+--------+--------------+--------------+-----------+----------+
</span></span><span style="display:flex;"><span>|vvbbcc|hhmm_int|abc_name_index|abc_page_index|bbbbb_index|dddd_index|
</span></span><span style="display:flex;"><span>+------+--------+--------------+--------------+-----------+----------+
</span></span><span style="display:flex;"><span>|    31|     943|           2.0|           2.0|        0.0|       0.0|
</span></span><span style="display:flex;"><span>|    18|     395|           6.0|           6.0|        0.0|      31.0|
</span></span><span style="display:flex;"><span>|     9|    1175|           2.0|           2.0|        0.0|      20.0|
</span></span><span style="display:flex;"><span>|    15|    1250|           5.0|           5.0|        0.0|      34.0|
</span></span><span style="display:flex;"><span>|    16|     266|           7.0|           7.0|        0.0|       1.0|
</span></span><span style="display:flex;"><span>|     6|     131|           5.0|           5.0|        0.0|       8.0|
</span></span><span style="display:flex;"><span>|     8|     136|           4.0|           3.0|        0.0|       0.0|
</span></span><span style="display:flex;"><span>|     7|    1229|           1.0|           1.0|        0.0|       5.0|
</span></span><span style="display:flex;"><span>|    10|      79|           4.0|           3.0|        0.0|       4.0|
</span></span><span style="display:flex;"><span>|    19|    1156|           6.0|           6.0|        0.0|       3.0|
</span></span><span style="display:flex;"><span>+------+--------+--------------+--------------+-----------+----------+
</span></span></code></pre></div><p>only showing top 10 rows</p>
<h3 id="验证scala-spark">验证scala spark<a href="#验证scala-spark" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>读取csv文件</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>    val spark <span style="color:#f92672">=</span> SparkSession.builder<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>      .appName<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;ShowLocal&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>      .config<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;spark.master&#34;</span>, <span style="color:#e6db74">&#34;local&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>      .enableHiveSupport<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>      .getOrCreate<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>    val path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;hdfs://sybigdata/user/example/part-00000-d3d3436e-f258-43b8-bfec-e45eb966edc7-c000.csv&#34;</span>
</span></span><span style="display:flex;"><span>    // 从HDFS中读取所有CSV文件
</span></span><span style="display:flex;"><span>    val df <span style="color:#f92672">=</span> spark.read.format<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;csv&#34;</span><span style="color:#f92672">)</span>.option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;header&#34;</span>, <span style="color:#e6db74">&#34;true&#34;</span><span style="color:#f92672">)</span>.load<span style="color:#f92672">(</span>path<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    // 打印数据
</span></span><span style="display:flex;"><span>    df.show<span style="color:#f92672">(</span>10<span style="color:#f92672">)</span>
</span></span></code></pre></div><p>请注意，要以windows 跑成功spark程序，以下配置是必须的
windows 作为单机跑起来，在本地运行</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## 在windows 本地以下为 本地运行spark, 以下代码是必须的</span>
</span></span><span style="display:flex;"><span>    System.setProperty<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;hadoop.home.dir&#34;</span>, <span style="color:#e6db74">&#34;C:\\winutils\\&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    val spark <span style="color:#f92672">=</span> SparkSession.builder<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>      .appName<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;helloSpsark&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>      .config<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;spark.master&#34;</span>, <span style="color:#e6db74">&#34;local&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>      .enableHiveSupport<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>      .getOrCreate<span style="color:#f92672">()</span>
</span></span></code></pre></div><p>写入csv文件</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>    System.setProperty<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;hadoop.home.dir&#34;</span>, <span style="color:#e6db74">&#34;C:\\winutils\\&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    val user <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;admin&#34;</span>
</span></span><span style="display:flex;"><span>    val ugi <span style="color:#f92672">=</span> UserGroupInformation.createRemoteUser<span style="color:#f92672">(</span>user<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    ugi.doAs<span style="color:#f92672">(</span>new PrivilegedExceptionAction<span style="color:#f92672">[</span>Unit<span style="color:#f92672">]()</span> <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>      override def run: Unit <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>        val spark <span style="color:#f92672">=</span> SparkSession.builder<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>          .appName<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;helloSpsark&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>          .config<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;spark.master&#34;</span>, <span style="color:#e6db74">&#34;local&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>          .enableHiveSupport<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>          .getOrCreate<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>        val tuples <span style="color:#f92672">=</span> Seq<span style="color:#f92672">((</span>1, <span style="color:#e6db74">&#34;spark&#34;</span><span style="color:#f92672">)</span>, <span style="color:#f92672">(</span>2, <span style="color:#e6db74">&#34;Big Data&#34;</span><span style="color:#f92672">))</span>
</span></span><span style="display:flex;"><span>        val df <span style="color:#f92672">=</span> spark.createDataFrame<span style="color:#f92672">(</span>tuples<span style="color:#f92672">)</span>.toDF<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>        df.show<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#这里会把csv文件保存在e:\abcd目录下</span>
</span></span><span style="display:flex;"><span>        df.write.mode<span style="color:#f92672">(</span>SaveMode.Overwrite<span style="color:#f92672">)</span>.format<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;csv&#34;</span><span style="color:#f92672">)</span>.save<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;file:///e:/abcd&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>        spark.stop<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">})</span>
</span></span></code></pre></div><p>以下是保存的csv文件目录</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>E:<span style="color:#ae81ff">\a</span>bcd&gt;dir 
</span></span><span style="display:flex;"><span>2023/05/11  14:59    &lt;DIR&gt;          .
</span></span><span style="display:flex;"><span>2023/05/11  14:59    &lt;DIR&gt;          ..
</span></span><span style="display:flex;"><span>2023/05/11  14:59                <span style="color:#ae81ff">12</span> .part-00000-8430568a-02c2-4db8-ad6c-16a2aed410df-c000.csv.crc
</span></span><span style="display:flex;"><span>2023/05/11  14:59                 <span style="color:#ae81ff">8</span> ._SUCCESS.crc
</span></span><span style="display:flex;"><span>2023/05/11  14:59                <span style="color:#ae81ff">21</span> part-00000-8430568a-02c2-4db8-ad6c-16a2aed410df-c000.csv
</span></span><span style="display:flex;"><span>2023/05/11  14:59                 <span style="color:#ae81ff">0</span> _SUCCESS
</span></span><span style="display:flex;"><span>               <span style="color:#ae81ff">4</span> 个文件             <span style="color:#ae81ff">41</span> 字节
</span></span><span style="display:flex;"><span>               <span style="color:#ae81ff">2</span> 个目录 211,274,485,760 可用字节
</span></span></code></pre></div><p>csv 文件内容</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>E:<span style="color:#ae81ff">\a</span>bcd&gt;type part-00000-8430568a-02c2-4db8-ad6c-16a2aed410df-c000.csv
</span></span><span style="display:flex;"><span>1,spark
</span></span><span style="display:flex;"><span>2,Big Data
</span></span></code></pre></div><h2 id="在windows10-用intellj-idea-创建spark程序步骤">在windows10 用intellj idea 创建spark程序步骤<a href="#在windows10-用intellj-idea-创建spark程序步骤" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>使用 &ldquo;net.alchim31.maven:scala-archetype-simple&rdquo;。这个 archetype 为您提供了一个基本的 Scala 项目结构，您可以在此基础上添加 Spark 依赖。</p>
<p>按照以下步骤在 IntelliJ IDEA 中创建一个基于 Scala + Spark 的 Maven 项目：</p>
<p>打开 IntelliJ IDEA，点击 &ldquo;Create New Project&rdquo;。</p>
<p>在左侧导航栏中选择 &ldquo;Maven&rdquo;，然后勾选 &ldquo;Create from archetype&rdquo;。</p>
<p>如果 &ldquo;net.alchim31.maven:scala-archetype-simple&rdquo; 不在列表中，点击 &ldquo;Add Archetype&rdquo; 按钮。</p>
<p>在 &ldquo;Add Archetype&rdquo; 对话框中输入以下信息：</p>
<p>GroupId: <code>net.alchim31.maven</code><br>
ArtifactId: <code>scala-archetype-simple</code><br>
Version: <code>1.7</code>（或您想使用的其他版本）<br>
点击 &ldquo;OK&rdquo;。</p>
<p>在列表中选择 &ldquo;net.alchim31.maven:scala-archetype-simple&rdquo;，然后点击 &ldquo;Next&rdquo;。</p>
<p>输入 &ldquo;GroupId&rdquo; 和 &ldquo;ArtifactId&rdquo;（例如，com.example 和 voicepredictionmodel）。点击 &ldquo;Next&rdquo;。</p>
<p>选择项目的存储位置，然后点击 &ldquo;Finish&rdquo;。</p>
<p>现在，您已经创建了一个基于 Scala 的 Maven 项目。接下来，需要为项目添加 Spark 依赖。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>&lt;dependency&gt;
</span></span><span style="display:flex;"><span>    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
</span></span><span style="display:flex;"><span>    &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt;
</span></span><span style="display:flex;"><span>    &lt;version&gt;3.2.0&lt;/version&gt;
</span></span><span style="display:flex;"><span>&lt;/dependency&gt;
</span></span><span style="display:flex;"><span>&lt;dependency&gt;
</span></span><span style="display:flex;"><span>    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
</span></span><span style="display:flex;"><span>    &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;
</span></span><span style="display:flex;"><span>    &lt;version&gt;3.2.0&lt;/version&gt;
</span></span><span style="display:flex;"><span>&lt;/dependency&gt;
</span></span></code></pre></div>
      </div></div>

  

  


<script src="https://utteranc.es/client.js"
        repo="ltanme/ltanme.github.io"
        issue-term="pathname"
        theme="github-dark-orange"
        crossorigin="anonymous"
        async>
</script>

</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2025 Powered by <a href="http://gohugo.io">Hugo</a></span>
    
        <span>:: Theme made by <a href="https://twitter.com/panr">panr</a></span>
      </div>
  </div>
</footer>

<script src="http://localhost:55668/assets/main.js"></script>
<script src="http://localhost:55668/assets/prism.js"></script>







<script async src="https://www.googletagmanager.com/gtag/js?id=G-M6YN54PT6M"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-M6YN54PT6M');
</script>

  
</div>

</body>
</html>
