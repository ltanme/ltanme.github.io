<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>How to Install Apache Spark on Windows Setup PySpark :: LTAN.ME</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="为什么要在Windows 10上运行Scala Spark程序 开发环境设置简单： 对于许多开发者来说，Windows是他们最熟悉的操作系统， 因此在Windows上进行开发可以节省大量的环境设置和配置时间。 此" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://ltan.me/post/2023/05/how-to-install-apache-spark-on-windows-setup-pyspark/" />




<link rel="stylesheet" href="https://ltan.me/assets/style.css">






<link rel="apple-touch-icon" href="https://ltan.me/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="https://ltan.me/img/favicon/orange.png">



<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="How to Install Apache Spark on Windows Setup PySpark">
<meta property="og:description" content="为什么要在Windows 10上运行Scala Spark程序 开发环境设置简单： 对于许多开发者来说，Windows是他们最熟悉的操作系统， 因此在Windows上进行开发可以节省大量的环境设置和配置时间。 此" />
<meta property="og:url" content="https://ltan.me/post/2023/05/how-to-install-apache-spark-on-windows-setup-pyspark/" />
<meta property="og:site_name" content="LTAN.ME" />

  
    <meta property="og:image" content="https://ltan.me/img/favicon/orange.png">
  

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">


  <meta property="article:published_time" content="2023-05-11 15:25:41 &#43;0800 CST" />












</head>
<body class="orange">


<div class="container center headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    LTAN.ME
  </div>
</a>

    </div>
    
    <div class="menu-trigger">menu</div>
    
  </div>
  
  <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/">Home</a></li>
        
      
        
          <li><a href="/post/">Archives</a></li>
        
      
        
          <li><a href="/tags/">Tags</a></li>
        
      
        
          <li><a href="/about/">About</a></li>
        
      
      
    

    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/">Home</a></li>
      
    
      
        <li><a href="/post/">Archives</a></li>
      
    
      
        <li><a href="/tags/">Tags</a></li>
      
    
      
        <li><a href="/about/">About</a></li>
      
    
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="https://ltan.me/post/2023/05/how-to-install-apache-spark-on-windows-setup-pyspark/">How to Install Apache Spark on Windows Setup PySpark</a></h1>
  <div class="post-meta">
    
      <span class="post-date">
        2023-05-11 [Mod: 2023-05-11]
      </span>
    
    
  </div>

  
  <span class="post-tags">
    
    #<a href="https://ltan.me/tags/python3/">python3</a>&nbsp;
    
    #<a href="https://ltan.me/tags/pyspark/">pyspark</a>&nbsp;
    
    #<a href="https://ltan.me/tags/scala/">scala</a>&nbsp;
    
    #<a href="https://ltan.me/tags/winutils/">winutils</a>&nbsp;
    
  </span>
  

  

  

  <div class="post-content"><div>
        <h2 id="为什么要在windows-10上运行scala-spark程序">为什么要在Windows 10上运行Scala Spark程序<a href="#为什么要在windows-10上运行scala-spark程序" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<h3 id="开发环境设置简单">开发环境设置简单：<a href="#开发环境设置简单" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>对于许多开发者来说，Windows是他们最熟悉的操作系统，
因此在Windows上进行开发可以节省大量的环境设置和配置时间。
此外，Windows上有IntelliJ IDEA可以方便Scala和Spark的开发。</p>
<h3 id="本地测试方便">本地测试方便：<a href="#本地测试方便" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>在本地Windows环境中进行开发，
可以方便快速的进行代码的单元测试和调试。
虽然Spark在集群中运行时的行为可能与在单个机器上有所不同，
但对于许多常见的任务，
本地测试通常可以提供足够的保证。</p>
<h2 id="如何做">如何做<a href="#如何做" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<h3 id="步骤1下载spark并安装">步骤1，下载spark并安装<a href="#步骤1下载spark并安装" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>根据自己的环境版本，在官网下载<code>spark-3.2.4-bin-hadoop2.7</code>包
下载链接地址为<code>https://spark.apache.org/downloads.html</code>
解压.tgz文件放到你本地目录夹,如<code>d:\spark-2.4.4-bin-hadoop2.7</code></p>
<h3 id="步骤2-下载winutils并安装">步骤2 下载winutils并安装<a href="#步骤2-下载winutils并安装" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>下载winutils.exe(解释windows hadoop通信问题)
下载链接地址<code>https://github.com/steveloughran/winutils/blob/master/hadoop-2.7.1/bin/winutils.exe</code>
存放到<code>d:\spark-2.4.4-bin-hadoop2.7\bin\winutils.exe</code> 该目录下</p>
<h3 id="步骤3设置环境变量">步骤3，设置环境变量<a href="#步骤3设置环境变量" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>设置windows环境变量</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#75715e"># Environment variable: </span>
SPARK_HOME<span style="color:#f92672">=</span>d:<span style="color:#ae81ff">\s</span>park-2.4.4-bin-hadoop2.7
HADOOP_HOME<span style="color:#f92672">=</span>d:<span style="color:#ae81ff">\s</span>park-2.4.4-bin-hadoop2.7
<span style="color:#75715e"># PATH variable:</span>
 d:<span style="color:#ae81ff">\s</span>park-2.4.4-bin-hadoop2.7<span style="color:#ae81ff">\b</span>in
</code></pre></div><h3 id="步骤4-验证测试scala-spark--pyspark">步骤4 验证测试scala spark 、 pyspark<a href="#步骤4-验证测试scala-spark--pyspark" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>测试 pyspark</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> sys
<span style="color:#75715e"># 这里不需要用pip install pyspark 会容易报错，根本不好安装的，不好使还不如这样引用</span>
sys<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#34;D:\spark-3.2.4-bin-hadoop2.7\python&#34;</span>)
<span style="color:#f92672">from</span> pyspark.sql <span style="color:#f92672">import</span> SparkSession

<span style="color:#75715e"># 创建 SparkSession</span>
spark <span style="color:#f92672">=</span> SparkSession<span style="color:#f92672">.</span>builder
    <span style="color:#f92672">.</span>appName(<span style="color:#e6db74">&#34;HDFS Read Example&#34;</span>)
    <span style="color:#f92672">.</span>config(<span style="color:#e6db74">&#34;spark.master&#34;</span>, <span style="color:#e6db74">&#34;local&#34;</span>)
    <span style="color:#f92672">.</span>getOrCreate()

<span style="color:#75715e"># 读取 CSV 文件</span>
df <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>read<span style="color:#f92672">.</span>csv(<span style="color:#e6db74">&#34;hdfs://sybigdata/user/example/part-00000-d3d3436e-f258-43b8-bfec-e45eb966edc7-c000.csv&#34;</span>, header<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)

<span style="color:#75715e"># 显示前 10 行</span>
df<span style="color:#f92672">.</span>show(<span style="color:#ae81ff">10</span>)

</code></pre></div><p>查看显示结果</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">Using Spark<span style="color:#960050;background-color:#1e0010">&#39;</span>s default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to <span style="color:#e6db74">&#34;WARN&#34;</span>.
To adjust logging level use sc.setLogLevel<span style="color:#f92672">(</span>newLevel<span style="color:#f92672">)</span>. For SparkR, use setLogLevel<span style="color:#f92672">(</span>newLevel<span style="color:#f92672">)</span>.
23/05/11 15:19:23 WARN NativeCodeLoader: Unable to load native-hadoop library <span style="color:#66d9ef">for</span> your platform... using builtin-java classes where applicable
+------+--------+--------------+--------------+-----------+----------+
|vvbbcc|hhmm_int|abc_name_index|abc_page_index|bbbbb_index|dddd_index|
+------+--------+--------------+--------------+-----------+----------+
|    31|     943|           2.0|           2.0|        0.0|       0.0|
|    18|     395|           6.0|           6.0|        0.0|      31.0|
|     9|    1175|           2.0|           2.0|        0.0|      20.0|
|    15|    1250|           5.0|           5.0|        0.0|      34.0|
|    16|     266|           7.0|           7.0|        0.0|       1.0|
|     6|     131|           5.0|           5.0|        0.0|       8.0|
|     8|     136|           4.0|           3.0|        0.0|       0.0|
|     7|    1229|           1.0|           1.0|        0.0|       5.0|
|    10|      79|           4.0|           3.0|        0.0|       4.0|
|    19|    1156|           6.0|           6.0|        0.0|       3.0|
+------+--------+--------------+--------------+-----------+----------+
</code></pre></div><p>only showing top 10 rows</p>
<h3 id="验证scala-spark">验证scala spark<a href="#验证scala-spark" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>读取csv文件</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">    val spark <span style="color:#f92672">=</span> SparkSession.builder<span style="color:#f92672">()</span>
      .appName<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;ShowLocal&#34;</span><span style="color:#f92672">)</span>
      .config<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;spark.master&#34;</span>, <span style="color:#e6db74">&#34;local&#34;</span><span style="color:#f92672">)</span>
      .enableHiveSupport<span style="color:#f92672">()</span>
      .getOrCreate<span style="color:#f92672">()</span>
    val path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;hdfs://sybigdata/user/example/part-00000-d3d3436e-f258-43b8-bfec-e45eb966edc7-c000.csv&#34;</span>
    // 从HDFS中读取所有CSV文件
    val df <span style="color:#f92672">=</span> spark.read.format<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;csv&#34;</span><span style="color:#f92672">)</span>.option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;header&#34;</span>, <span style="color:#e6db74">&#34;true&#34;</span><span style="color:#f92672">)</span>.load<span style="color:#f92672">(</span>path<span style="color:#f92672">)</span>
    // 打印数据
    df.show<span style="color:#f92672">(</span>10<span style="color:#f92672">)</span>
</code></pre></div><p>请注意，要以windows 跑成功spark程序，以下配置是必须的
windows 作为单机跑起来，在本地运行</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">
<span style="color:#75715e">## 在windows 本地以下为 本地运行spark, 以下代码是必须的</span>
    System.setProperty<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;hadoop.home.dir&#34;</span>, <span style="color:#e6db74">&#34;C:\\winutils\\&#34;</span><span style="color:#f92672">)</span>

    val spark <span style="color:#f92672">=</span> SparkSession.builder<span style="color:#f92672">()</span>
      .appName<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;helloSpsark&#34;</span><span style="color:#f92672">)</span>
      .config<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;spark.master&#34;</span>, <span style="color:#e6db74">&#34;local&#34;</span><span style="color:#f92672">)</span>
      .enableHiveSupport<span style="color:#f92672">()</span>
      .getOrCreate<span style="color:#f92672">()</span>
</code></pre></div><p>写入csv文件</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">    System.setProperty<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;hadoop.home.dir&#34;</span>, <span style="color:#e6db74">&#34;C:\\winutils\\&#34;</span><span style="color:#f92672">)</span>

    val user <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;admin&#34;</span>
    val ugi <span style="color:#f92672">=</span> UserGroupInformation.createRemoteUser<span style="color:#f92672">(</span>user<span style="color:#f92672">)</span>

    ugi.doAs<span style="color:#f92672">(</span>new PrivilegedExceptionAction<span style="color:#f92672">[</span>Unit<span style="color:#f92672">]()</span> <span style="color:#f92672">{</span>
      override def run: Unit <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>
        val spark <span style="color:#f92672">=</span> SparkSession.builder<span style="color:#f92672">()</span>
          .appName<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;helloSpsark&#34;</span><span style="color:#f92672">)</span>
          .config<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;spark.master&#34;</span>, <span style="color:#e6db74">&#34;local&#34;</span><span style="color:#f92672">)</span>
          .enableHiveSupport<span style="color:#f92672">()</span>
          .getOrCreate<span style="color:#f92672">()</span>
        val tuples <span style="color:#f92672">=</span> Seq<span style="color:#f92672">((</span>1, <span style="color:#e6db74">&#34;spark&#34;</span><span style="color:#f92672">)</span>, <span style="color:#f92672">(</span>2, <span style="color:#e6db74">&#34;Big Data&#34;</span><span style="color:#f92672">))</span>
        val df <span style="color:#f92672">=</span> spark.createDataFrame<span style="color:#f92672">(</span>tuples<span style="color:#f92672">)</span>.toDF<span style="color:#f92672">()</span>
        df.show<span style="color:#f92672">()</span>
        <span style="color:#75715e">#这里会把csv文件保存在e:\abcd目录下</span>
        df.write.mode<span style="color:#f92672">(</span>SaveMode.Overwrite<span style="color:#f92672">)</span>.format<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;csv&#34;</span><span style="color:#f92672">)</span>.save<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;file:///e:/abcd&#34;</span><span style="color:#f92672">)</span>
        spark.stop<span style="color:#f92672">()</span>
      <span style="color:#f92672">}</span>
    <span style="color:#f92672">})</span>
</code></pre></div><p>以下是保存的csv文件目录</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">E:<span style="color:#ae81ff">\a</span>bcd&gt;dir 
2023/05/11  14:59    &lt;DIR&gt;          .
2023/05/11  14:59    &lt;DIR&gt;          ..
2023/05/11  14:59                <span style="color:#ae81ff">12</span> .part-00000-8430568a-02c2-4db8-ad6c-16a2aed410df-c000.csv.crc
2023/05/11  14:59                 <span style="color:#ae81ff">8</span> ._SUCCESS.crc
2023/05/11  14:59                <span style="color:#ae81ff">21</span> part-00000-8430568a-02c2-4db8-ad6c-16a2aed410df-c000.csv
2023/05/11  14:59                 <span style="color:#ae81ff">0</span> _SUCCESS
               <span style="color:#ae81ff">4</span> 个文件             <span style="color:#ae81ff">41</span> 字节
               <span style="color:#ae81ff">2</span> 个目录 211,274,485,760 可用字节
</code></pre></div><p>csv 文件内容</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">E:<span style="color:#ae81ff">\a</span>bcd&gt;type part-00000-8430568a-02c2-4db8-ad6c-16a2aed410df-c000.csv
1,spark
2,Big Data
</code></pre></div><h2 id="在windows10-用intellj-idea-创建spark程序步骤">在windows10 用intellj idea 创建spark程序步骤<a href="#在windows10-用intellj-idea-创建spark程序步骤" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>使用 &ldquo;net.alchim31.maven:scala-archetype-simple&rdquo;。这个 archetype 为您提供了一个基本的 Scala 项目结构，您可以在此基础上添加 Spark 依赖。</p>
<p>按照以下步骤在 IntelliJ IDEA 中创建一个基于 Scala + Spark 的 Maven 项目：</p>
<p>打开 IntelliJ IDEA，点击 &ldquo;Create New Project&rdquo;。</p>
<p>在左侧导航栏中选择 &ldquo;Maven&rdquo;，然后勾选 &ldquo;Create from archetype&rdquo;。</p>
<p>如果 &ldquo;net.alchim31.maven:scala-archetype-simple&rdquo; 不在列表中，点击 &ldquo;Add Archetype&rdquo; 按钮。</p>
<p>在 &ldquo;Add Archetype&rdquo; 对话框中输入以下信息：</p>
<p>GroupId: <code>net.alchim31.maven</code><br>
ArtifactId: <code>scala-archetype-simple</code><br>
Version: <code>1.7</code>（或您想使用的其他版本）<br>
点击 &ldquo;OK&rdquo;。</p>
<p>在列表中选择 &ldquo;net.alchim31.maven:scala-archetype-simple&rdquo;，然后点击 &ldquo;Next&rdquo;。</p>
<p>输入 &ldquo;GroupId&rdquo; 和 &ldquo;ArtifactId&rdquo;（例如，com.example 和 voicepredictionmodel）。点击 &ldquo;Next&rdquo;。</p>
<p>选择项目的存储位置，然后点击 &ldquo;Finish&rdquo;。</p>
<p>现在，您已经创建了一个基于 Scala 的 Maven 项目。接下来，需要为项目添加 Spark 依赖。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt;
    &lt;version&gt;3.2.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;
    &lt;version&gt;3.2.0&lt;/version&gt;
&lt;/dependency&gt;

</code></pre></div>
      </div></div>

  

  


<script src="https://utteranc.es/client.js"
        repo="ltanme/ltanme.github.io"
        issue-term="pathname"
        theme="github-dark-orange"
        crossorigin="anonymous"
        async>
</script>

</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2023 Powered by <a href="http://gohugo.io">Hugo</a></span>
    
        <span>:: Theme made by <a href="https://twitter.com/panr">panr</a></span>
      </div>
  </div>
</footer>

<script src="https://ltan.me/assets/main.js"></script>
<script src="https://ltan.me/assets/prism.js"></script>







<script async src="https://www.googletagmanager.com/gtag/js?id=G-M6YN54PT6M"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-M6YN54PT6M');
</script>

  
</div>

</body>
</html>
